C:\Users\sari\Desktop\DeepCover\models\A2C\train.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  state = torch.tensor(state)
C:\Users\sari\Desktop\DeepCover\models\A2C\train.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  s1 = torch.tensor(s1)
C:\Users\sari\Desktop\DeepCover\models\A2C\train.py:65: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss_critic = F.smooth_l1_loss(y_predicted, y_expected)
episode num: 0
reward for step 500 is [1393727.8]
reward for step 1000 is [6323968.]
----------Target Models Saved Successfully----------
episode num: 1
reward for step 500 is [1393727.8]
reward for step 1000 is [6323968.]
episode num: 2
reward for step 500 is [1393727.8]
reward for step 1000 is [6323968.]
episode num: 3
Traceback (most recent call last):
  File "C:\Users\sari\Desktop\DeepCover\main.py", line 77, in <module>
    main()
  File "C:\Users\sari\Desktop\DeepCover\main.py", line 62, in main
    trainer.optimize(obs, action, reward, next_obs, args.gamma, args.tau)
  File "C:\Users\sari\Desktop\DeepCover\models\A2C\train.py", line 75, in optimize
    self.actor_optimizer.step()
  File "C:\Users\sari\AppData\Local\anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "C:\Users\sari\AppData\Local\anaconda3\lib\site-packages\torch\optim\optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "C:\Users\sari\AppData\Local\anaconda3\lib\site-packages\torch\optim\adam.py", line 141, in step
    adam(
  File "C:\Users\sari\AppData\Local\anaconda3\lib\site-packages\torch\optim\adam.py", line 281, in adam
    func(params,
  File "C:\Users\sari\AppData\Local\anaconda3\lib\site-packages\torch\optim\adam.py", line 391, in _single_tensor_adam
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
KeyboardInterrupt